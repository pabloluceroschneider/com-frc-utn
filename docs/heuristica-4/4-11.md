| Un código dado tiene seis mensajes con probabilidades $P(A)=\frac{1}{2}; P(B) =\frac{1}{4}; P(C)=\frac{1}{8}; P(D)=\frac{1}{16}; P(E)=\frac{1}{32}; P(F)=\frac{1}{32}$. |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| a. Determinar la entropía del código.                                                                                                                                   |
| b. Suponiendo independencia estadística, encontrar la información en los mensajes: $b_1)X_1=BABC$. $b_2)X_2=FDEF$                                                       |
| c. Repetir los puntos a) y b) suponiendo que $P(A)=0.25; P(B)=0.22; P(C)= 0.18; P(D)=0.15; P(E)=0.12 yP(F) =0.08.$                                                      |

Fórmula de entropía:

$$
H(S) = \sum_{i=1} ^{m} p_i * log_2(\frac{1}{p_i})
$$

**a)**

$$
H(S) = P(A)*I_A + P(B)*I_B + P(C)*I_C + P(D)*I_D + P(E)*I_E + P(F)*I_F
$$

$$
H(S) = (\frac{1}{2} * log_2(\frac{1}{\frac{1}{2}})) + (\frac{1}{4} * log_2(\frac{1}{\frac{1}{4}})) + (\frac{1}{8} * log_2(\frac{1}{\frac{1}{8}})) + (\frac{1}{16} * log_2(\frac{1}{\frac{1}{16}})) + (\frac{1}{32} * log_2(\frac{1}{\frac{1}{32}})) + (\frac{1}{32} * log_2(\frac{1}{\frac{1}{32}}))
$$

$$
H(S) = 1,9375\ \frac{bits}{símbolo}
$$

> a. La entropía del código es $1,9375\ \frac{bits}{símbolo}$.

**b.1)**

Suponiendo independencia estadística;

$$
I_{BABC} = I_B + I_A + I_B + I_C
$$

$$
I_{BABC} = log_2(\frac{1}{1/4}) + log_2(\frac{1}{1/2}) + log_2(\frac{1}{1/4}) + log_2(\frac{1}{1/8})
$$

$$
I_{BABC} = 2\ bits + 1\ bit + 1\ bits + 3\ bits = 8\ bits
$$

> b.1) La información de $I_{BABC}$ es de $8\ bits$.

**b.2)**

\# $P(A)=\frac{1}{2}, P(B) =\frac{1}{4}; P(C)=\frac{1}{8}, P(D)=\frac{1}{16}, P(E)=\frac{1}{32}, P(F)=\frac{1}{32}$

Suponiendo independencia estadística;

$$
I_{FDEF} = I_F + I_D + I_E + I_F
$$

$$
I_{FDEF} = log_2(\frac{1}{1/32}) + log_2(\frac{1}{1/16}) + log_2(\frac{1}{1/32}) + log_2(\frac{1}{1/32})
$$

$$
I_{FDEF} = 5\ bits + 4\ bit + 5\ bits + 5\ bits = 19\ bits
$$

> b.2) La información de $I_{FDEF}$ es de $19\ bits$.

**c)**

$$
H(S) = (0.25* log_2(\frac{1}{0.25})) + (0.22 * log_2(\frac{1}{0.22})) + (0.18 * log_2(\frac{1}{0.18})) + (0.15 * log_2(\frac{1}{0.15})) + (0.12 * log_2(\frac{1}{0.12})) + (0.08 * log_2(\frac{1}{0.08}))
$$

$$
H(S) = 2,95\ \frac{bits}{símbolo}
$$

> a') La entropía del código es $2,95\ \frac{bits}{símbolo}$.

$$
I_{BABC} = log_2(\frac{1}{0.22}) + log_2(\frac{1}{0.25}) + log_2(\frac{1}{0.22}) + log_2(\frac{1}{0.18})
$$

$$
I_{BABC} = 8,843\ bits
$$

> b.1') La información de $I_{BABC}$ es de $8,843\ bits$.

$$
I_{FDEF} = log_2(\frac{1}{0.08}) + log_2(\frac{1}{0.15}) + log_2(\frac{1}{0.12}) + log_2(\frac{1}{0.08})
$$

$$
I_{FDEF} = 13,084\ bits
$$

> b.2') La información de $I_{FDEF}$ es de $13,084\ bits$.
