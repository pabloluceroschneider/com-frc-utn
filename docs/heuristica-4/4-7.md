| Una fuente posee los símbolos A, B, C y D, con probabilidades 1⁄2, 1⁄4, 1/8 y 1/8 respectivamente. Se solicita:    |
| ------------------------------------------------------------------------------------------------------------------ |
| a. Calcular la información del mensaje de tres símbolos X=BDA, suponiendo que son estadísticamente independientes. |
| b. Calcular la entropía del mensaje X = BDA                                                                        |
| c. Determine la información promedio por símbolo para el código de los cuatro eventos A, B, C y D.                 |
| d. Determine la entropía si los cuatro símbolos fueran equiprobables.                                              |

a)

$$
P(A) = \frac{1}{2} = 0,5   \rightarrow   I_A = log_2(\frac{1}{0,5}) = 1 bit
$$

$$
P(B) = \frac{1}{4} = 0,25   \rightarrow   I_B = log_2(\frac{1}{0,25}) = 2 bit
$$

$$
P(C) = \frac{1}{8} = 0,125   \rightarrow   I_C = log_2(\frac{1}{0,125}) = 3 bit
$$

$$
P(C) = \frac{1}{8} = 0,125   \rightarrow   I_C = log_2(\frac{1}{0,125}) = 3 bit
$$

$$
I_{BDA} = I_B + I_D + I_A = 2 bits + 3 bits + 1 bit = 6 bits
$$

> La información de los símbolos BDA es de 6 bits.

b)

$$
H(S)_{BDA} = ( \frac{1}{4} * 2\ bits ) + ( \frac{1}{8} * 3\ bits ) + ( \frac{1}{2} * 1\ bit ) = 1,375 \frac{bits}{símbolo}
$$

> La entropía del mensaje BDA es $ 1,375 \frac{bits}{símbolo} $

c)

$$
H(S)_{ACBD} = ( \frac{1}{2} * 1\ bit ) + ( \frac{1}{4} * 2\ bits ) + ( \frac{1}{8} * 3\ bits ) + ( \frac{1}{8} * 3\ bits )
$$

$$
H(S)_{ACBD} = \frac{7}{4} = 1,75 \frac{bits}{símbolo}
$$

> La entropía del mensaje ACBD es $ 1,75 \frac{bits}{símbolo} $

d)

Las fuentes equiprobables tienen entropía igual a $ log_2 = m $

$$
m = 4   \rightarrow  log_2 = 4   \rightarrow  H = 2 \frac{bits}{símbolo}
$$

> Si los 4 símbolos fueran equiprobables, la entropía sería $H = 2 \frac{bits}{símbolo}$
