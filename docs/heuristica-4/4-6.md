| Una fuente produce 5 símbolos con probabilidades 1⁄2, 1⁄4, 1/8, 1/16 y 1/16. Calcular la información promedio o entropía H. |
| --------------------------------------------------------------------------------------------------------------------------- |

$$
P(S_1) = \frac{1}{2}  \qquad \qquad \rightarrow \qquad \qquad I_{S_1} = log_2(\frac{1}{0,5}) = 1 bit
$$

$$
P(S_2) = \frac{1}{4}  \qquad \qquad \rightarrow \qquad \qquad I_{S_2} = log_2(\frac{1}{0,25}) = 2 bit
$$

$$
P(S_3) = \frac{1}{8}  \qquad \qquad \rightarrow \qquad \qquad I_{S_3} = log_2(\frac{1}{0,125}) = 3 bit
$$

$$
P(S_4) = \frac{1}{16}  \qquad \qquad \rightarrow \qquad \qquad I_{S_4} = log_2(\frac{1}{\frac{1}{16}}) = 4 bit
$$

$$
P(S_5) = \frac{1}{16}  \qquad \qquad \rightarrow \qquad \qquad I_{S_5} = log_2(\frac{1}{\frac{1}{16}}) = 4 bit
$$

#### Cálculo de entropía

$$
H(S) = E \{ I(S_m) \} = \sum_{i=1} ^{m} p_i . I(S_i)
$$

$$
H(S) = ( \frac{1}{2} * 1 bit ) + ( \frac{1}{4} * 4 bit ) + ( \frac{1}{8} * 3 bit ) + ( \frac{1}{16} * 4 bit ) + + ( \frac{1}{16} * 4 bit )
$$

$$
H(S) = 1,875 \frac{bits}{símbolo}
$$

> La entropía o información promedio de la fuente es de $ 1,875 \frac{bits}{símbolo} $
